
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AssetField</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <link rel="icon" type="image/png" href="../img/newyork.ico">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                AssetField: Assets Mining and Reconfiguration in  <br> Ground Feature Plane Representation
            </h1>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <div style="margin-bottom: 0.7em; margin-top:0.2em" class="authors">
                    <a style="color:#000000;" href="https://kam1107.github.io/">Yuanbo Xiangli*<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://eveneveno.github.io/lnxu/">Linning Xu*<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://xingangpan.github.io/">Xingang Pan<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="http://nxzhao.com/">Nanxuan Zhao<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="http://daibo.info/">Bo Dai<sup>4</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="http://dahua.me/">Dahua Lin<sup>1,4</sup></a>
                </div>

                <div style="margin-bottom: 0.5em;" class="affiliations">
                    <a href="http://mmlab.ie.cuhk.edu.hk/">The Chinese University of Hong Kong<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.mpi-inf.mpg.de/">Max Planck Institute for Informatics<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    
                    </br>
                    <a href="https://www.bath.ac.uk/">University of Bath<sup>3</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
                    <a href="https://www.shlab.org.cn/">Shanghai Artificial Intelligence Laboratory<sup>4</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
                </div>

                <div style="margin-bottom: 0.7em;" class="col-md-12 text-center">
                    *denotes equal contribution
                </div>

            </div>
        </div>

        <div style="margin-bottom: 0.7em;" class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="./img/main.pdf">
                            <image src="../img/paper.png" height="50px"><br>
                                <h5><strong>Paper</strong></h5>
                            </a>
                        </li>
                        <li>
                            <image src="../img/github_pad.png" height="50px"><br>
                                <h5><strong>Code (coming soon)</strong></h5>
                            </a>
                        </li>
                        <li>
                            <a href="./img/supp.pdf">
                            <image src="../img/paperclip.png" height="50px"><br>
                                <h5><strong>Supplement</strong></h5>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="./img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Both indoor and outdoor environments are inherently
                    structured and repetitive. Traditional modeling pipelines
                    keep an asset library storing unique object templates, which
                    is both versatile and memory efficient in practice. Inspired
                    by this observation, we propose AssetField, a novel neural
                    scene representation that learns a set of object-aware
                    ground feature planes to represent the scene, where an asset
                    library storing template feature patches can be constructed
                    in an unsupervised manner. Unlike existing methods which
                    require object masks to query spatial points for object editing,
                    our ground feature plane representation offers a natural
                    visualization of the scene in the bird-eye view, allowing
                    a variety of operations (e.g. translation, duplication, deformation)
                    on objects to configure a new scene. With the template
                    feature patches, group editing is enabled for scenes
                    with many recurring items to avoid repetitive work on object
                    individuals. We show that AssetField not only achieves
                    competitive performance for novel-view synthesis but also
                    generates realistic renderings for new scene configurations.     
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Framework
                </h3>
                <image src="./img/pipeline.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    <strong> Overview of AssetField.</strong> 
                    (a) We demonstrate on a scene without background for clearer visuals. (b) The proposed ground
                    feature plane representation factorizes a neural field into a horizontal feature plane and a vertical feature axis. (c) We further integrate
                    color and semantic field into a 2D neural plane, which is decoded into 3D-aware features with the geometry guidance from scene density.
                    The inferred RGB-DINO plane is rich in object appearance and semantic clues whilst being less sensitive to vertical displacement between
                    objects, on which we can (d) detect assets and grouping them into categories. (e) For each category, we select a template object and store
                    its density and color ground feature patches into the asset library. A cross-scene asset library can be construct by letting different scenes fit
                    there own ground feature planes whilst sharing the same vertical feature axes and decoders/renderers.
                    <!-- The blue box<span style="background-color: #cfe2f3a8">(b)</span> and orange box<span style="background-color: #fce5cdb3">(c-d)</span> show the pipeline of Basic and Improved AssetField respectively.
                    Basic AssetField models a set of separate density and RGB (optionally DINO) fields. Improved AssetField unifies color and semantic field
                    into RGB-DINO ground feature plane that is decoded into 3D with geometry guidance from density occupancy. This further encourages
                    learning an object-centric RGB-DINO plane that is suitable for <span style="background-color: #d9d2e9a8">(e)</span> asset mining and layout discovery. The extracted category template
                    from the asset and layout discovery procedure is then added to the maintained asset library for future usage. -->
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Example Results on Synthetic Scenes
                </h3>
                    <h4>
                        # Delete Instances
                    </h4>
                    <image src="./img/results-deletion.png" class="img-responsive" alt="overview">
                    <h4>
                        # Move Instances
                    </h4>
                    <image src="./img/results-translation.png" class="img-responsive" alt="overview">
                    <h4>
                        # Manipulation (rescaling & translation)
                    </h4>
                    <image src="./img/results-move&rescale.png" class="img-responsive" alt="overview">
                    <h4>
                        # Manipulation (rotation & deletion)
                    </h4>
                    <image src="./img/results-rot&del.png" class="img-responsive" alt="overview">
                    <br>
                    <p class="text-justify">
                        <strong> Results of asset mining and scene editing with AssetField.</strong>
                        <span style="background-color: #fce5cdb3">(a)</span> Our approach learns informative density and RGB-DINO
                        ground feature planes that support object detection and categorization. <span style="background-color: #d9ead3a8">(b)</span> With joint training, an asset library can be constructed
                        by storing ground feature plane patches of the radiance field (we show label patches here for easy visualization). <span style="background-color: #cfe2f3a8">(c)</span> The proposed ground
                        plane representation provides an explicit visualization of the scene configuration, which be directly manipulated by users. The altered
                        ground feature plane is then fed to the global MLP renderer along with the shared z-axis feature to render views of the novel scenes. Basic
                        operations such as object removal, translation, rotation and rescaling are demonstrated on the right.
                    </p>
                <table>
                    <tr>
                        <td width="30%">
                            <video id="v11" width="100%" autoplay loop muted controls>
                                <source src="img/move.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td width="30%">
                            <video id="v12" width="100%" autoplay loop muted controls>
                                <source src="img/change_appearance.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td width="30%">
                            <video id="v12" width="100%" autoplay loop muted controls>
                                <source src="img/reconfig_room.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>                
            </div>
            <br>
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Example Results on Real-world Scenes
                </h3>
                    <image src="./img/realscenes.png" class="img-responsive" alt="overview">
                    <table>
                        <tr>
                            <td width="100%">
                                <video id="v11" width="100%" autoplay loop muted controls>
                                    <source src="img/garden_added_ball.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>
                    <p class="text-justify">
                        <strong> Example editings on Mip-NeRF360 and DoNeRF dataset.</strong>
                        Objects are identified on the RGB-DINO plane then replicated in the scene.
                        <strong>Video left: </strong> Novel view rendering of the original scene;
                        <strong>Video right: </strong> We replicate the metal ball in the back.
                    </p>
                    <br>
                    <image src="./img/toydesk_remove.png" class="img-responsive" alt="overview">
                    <table>
                        <tr>
                            <td width="100%">
                                <video id="v12" width="100%" autoplay loop muted controls>
                                    <source src="img/toydesk.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>     
                    <p class="text-justify">
                        <strong> Object removal on toydesk scene from ObjectNeRF.</strong>
                        Objects are first identified on each ground feature plane then substituted by the table
                        feature patches. We simply ‘crop’ a feature patch from the table
                        region and ‘paste’ it on to the object regions. Note that our method
                        can also remove the shadow along with the object, whereas Object-NeRF 
                        cannot and leaves a <span style="border-top:3px#FF0000 solid;border-bottom:3px#FF0000 solid;border-left:3px#FF0000 solid;border-right:3px#FF0000 solid;">black hole</span>.
                    </p>
                    <br>
                    <image src="./img/vertical_toydesk.png" class="img-responsive" alt="overview">
                    <p class="text-justify">
                        <strong> Cross-scene object insertion.</strong>
                        We insert the plastic bowl from toydesk1 to toydesk2. 
                        <strong>Vertical translation</strong> of the bowl is realized by jointly
                        modeling the aligned/lifted/sunken-toydesk1 with toydesk2, and
                        compose the new scene with feature patches inferred at different
                        scene elevation.
                    </p>           
            </div>
        </div>
        <br>
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Example Results on City Scenes (Google Earth)
            </h3>
            <image src="./img/cityscene.png" class="img-responsive" alt="overview">
            <table>
                <tr>
                    <td width="50%">
                        <video id="v11" width="100%" autoplay loop muted controls>
                            <source src="img/original_Colosseum_SagradaFamilia.mp4" type="video/mp4" />
                        </video>
                    </td>
                    <td width="50%">
                        <video id="v12" width="100%" autoplay loop muted controls>
                            <source src="img/composed.mp4" type="video/mp4" />
                        </video>
                    </td>
                </tr>
            </table>     
            <p class="text-justify">
                <strong>Editing two city scenes collected from Google Earth ©2023 Google. </strong> 
                AssetField is versatile where users can directly
                operate on the ground feature plane, supporting both within-scene
                and cross-scene editing, producing realistic rendering results.
                <strong>Left: </strong>
                Novel view rendering of the original Colosseum (Rome) and Sagrada Familia (Barcelona) scene.
                <strong>Right: </strong>
                Two Colosseum of different sizes are inserted into a park scene; Colosseum and Sangrada Familia are inserted into a park scene.
            </p>           
        </div>
    </div>
    <br>
        <!-- Slideshow container -->
       
<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Links
                </h3>
            </div>
        </div> -->

        

<!--         <div class="row" id="header_img">
            <figure class="col-md-8 col-md-offset-2">
                <image src="img/llff_teaser.png" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
                
        </div> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Technical Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/LY6MgDUzS3M" allowfullscreen style="pbungeeosition:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->
            
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{xiangli2022bungeenerf,
    title={BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering},
    author={Xiangli, Yuanbo and Xu, Linning and Pan, Xingang, and Zhao, Nanxuan and Rao, Anyi and Theobalt, Christian and Dai, Bo and Lin, Dahua},
    booktitle = {The European Conference on Computer Vision (ECCV)}, 
    year={2022}
}
                    </textarea>
                </div> -->
</body>

	<script type="text/javascript">
        var slideIndex = 1;
        showSlides(slideIndex);

        // Next/previous controls
        function plusSlides(n) {
        showSlides(slideIndex += n);
        }

        // Thumbnail image controls
        function currentSlide(n) {
        showSlides(slideIndex = n);
        }

        function showSlides(n) {
        var i;
        var slides = document.getElementsByClassName("mySlides");
        var dots = document.getElementsByClassName("dot");
        if (n > slides.length) {slideIndex = 1}
        if (n < 1) {slideIndex = slides.length}
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
        }
        slides[slideIndex-1].style.display = "block";
        dots[slideIndex-1].className += " active";
        }
	</script>




</html>
