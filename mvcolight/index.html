<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
    <meta name="description"
        content="MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation">
  <meta name="keywords" content="Object Compositing, Scene Relighting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- MathJax for LaTeX support -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation</h1>
          <div class="is-size-3 publication-authors">
            <span class="univerity-block">NIPS 2025</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://cskrren.github.io/">Kerui Ren</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=VmPQ6akAAAAJ&hl=zh-CN">Jiayang Bai</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://eveneveno.github.io/lnxu/">Linning Xu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://jianglh-whu.github.io/">Lihan Jiang</a><sup>2,5</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://oceanpang.github.io/">Jiangmiao Pang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://mulinyu.github.io/">Mulin Yu*</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://daibo.info/">Bo Dai*</a><sup>6</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Artificial Intelligence Laboratory,</span>
            <br>
            <span class="author-block"><sup>3</sup>Nanjing University,</span>
            <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong,</span>
            <br>
            <span class="author-block"><sup>5</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>6</sup>The University of Hong Kong</span>
          </div>
          <br>
          <div class="is-size-6 publication-authors">
            * denotes corresponding author.
          </div>

          <!-- <h5 class="title is-5 publication-title">NeurIPS D&B Track 2024</h5> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.21483"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Huggingface</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="text-container" style="margin-bottom: 1rem;">
      <p style="font-size:20px;">
        TL;DR: We introduce <b>MV-CoLight</b>, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. 
      </p>
    </div>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls height="100%">
        <source src="./static/videos/teaser_video.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
        <p>Achieving <b>Multi-view Harmonization</b> within a UE5-captured Scene</p> 
        <!-- <p><b>Videos may take a few seconds to load.</b></p> -->
      </h2>
    </div>
  </div>
</section>

<style>
.section {
    padding: 1rem 1.5rem;
}
</style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
          <br>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            We introduce <b>MV-CoLight</b>, a two-stage framework for illumination-consistent object compositing in both 2D images and 3D scenes. 
            Our novel <b>feed-forward</b> architecture models <b>lighting and shadows</b> directly, avoiding the iterative biases of diffusion-based methods. We employ a <b>Hilbert curve</b>-based mapping to align 2D image inputs with 3D Gaussian scene representations seamlessly. 
            To facilitate training and evaluation, we further introduce <b>a large-scale 3D compositing dataset</b>. 
            Experiments demonstrate <b>state-of-the-art</b> harmonized results across standard benchmarks and our dataset, as well as casually captured <b>real-world</b> scenes demonstrate the frameworkâ€™s robustness and wide generalization. 
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div style="text-align: center; margin: 1rem 0;">
            <img src="static/images/pipeline.png" alt="MY ALT TEXT"/>
          </div>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Overview of MV-CoLight.</b> (a) We insert <b>a  white puppy</b> as the composite object onto the table between basketballs, and render multi-view <b>inharmonious images</b>, <b>background-only images</b>, and <b>depth maps</b> using a camera trajectory moving from distant to close-up positions. (b) We input a single-view data into the 2D object compositing model, which processes the data through multiple <b>Swin Transformer</b> blocks to output the harmonized result. (c) We project the multi-view features from 2D models into Gaussian space via <b>\(\Phi(\cdot)\)</b>, combine them with the original inharmonious Gaussian colors projected into 2D Gaussian color space through <b>\(\Psi(\cdot)\)</b>, and then feed them into the 3D object compositing model. The model outputs harmonized Gaussian colors and computes rendering loss by incorporating Gaussian shape attributes.
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <div style="text-align: center; margin: 1rem 0;">
            <img src="static/images/dataset.png" alt="MY ALT TEXT"/>
          </div>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            Visualization of the <b>DTC-MultiLight</b> dataset. We showcase rendered results of diverse scenes created using objects from the DTC dataset within the Blender engine, highlighting multi-view perspectives and varying lighting conditions.
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <img src="static/images/single-view.png" alt="MY ALT TEXT"/>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Single-view qualitative comparisons.</b> Compared to baselines, our method successfully generates <b>coherence illumination</b> and <b>plausible shadows</b> while decoupling highlights from inserted objects.
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
        <div class="content has-text-justified">
          <img src="static/images/multi-view.png" alt="MY ALT TEXT"/>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Multi-view qualitative comparisons.</b> Our approach synthesizes <b>multi-view consistent illumination and shadows</b> while <b>strictly preserving</b> the original scene geometry, scale and object placement.
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
        <div class="content has-text-justified">
          <img src="static/images/real.png" alt="MY ALT TEXT"/>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Real-world scene visualization.</b> We evaluate our method on real-world scenes and achieve both <b>color harmonization</b> and <b>realistic lighting/shadow generation</b>. 
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
      </div>
    </div>
</section>

<section class="hero is-small" style="margin-top: 60px; margin-bottom: 50px;">
  <div class="columns is-centered ">
    <div class="column is-full-width">
      <h3 class="title is-3 has-text-centered" style="margin-bottom: 0px">Harmonized 3D Gaussians</h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- <h5 class="title is-5">We present the harmonized results on our test set.</h5> -->
      <!-- <p>All the cases shown below are test set results. Horizontal and vertical videos are generated by the same model weights. You can use the maximize button in the bottom right corner of the video to play the video in full screen mode to observe more details.</p> -->
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/00146.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/00151.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/00198.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/00361.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/00950.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/1687.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/2372.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <!-- <h2 class="title is-4"></h2> -->
      <!-- <p>All the cases shown below are test set results. Horizontal and vertical videos are generated by the same model weights. You can use the maximize button in the bottom right corner of the video to play the video in full screen mode to observe more details.</p> -->
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/2818.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/2886.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/2960.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/3047.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/3519.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/3849.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline>
            <source src="./static/videos/DTC/4406.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
</section>

<br>
<br>
<br>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Luminous Object Compositing</h2>
        <div class="content has-text-justified">
          <img src="static/images/single-light.png" alt="MY ALT TEXT"/>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Visual results of inserting a luminous object.</b> Our method simulates the <b>illumination effects</b> of luminous spheres within the scene environment. 
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
        <div class="content has-text-justified">
          <img src="static/images/multi-light.png" alt="MY ALT TEXT"/>
          <p style="line-height: 1.6; margin-bottom: 1.5rem;">
            <b>Multi-view visualization.</b> Our method meticulously simulates the <b>emission effects</b> of inserted light sources, their illumination on surrounding objects, and shadows.
          </p>
          <!-- <p>Should you have any enquiries, please contact the first author.</p> -->
        </div>
      </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!--
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the template of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
